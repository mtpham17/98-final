---
title: "Final Project EDA"
output: html_notebook
---


```{r}
library(mltools)
library(data.table)
library(dplyr)
library(stringr)
```

```{r}
data <- read.csv('data/immigration_policies/policy_list.csv')
# summary(data)

```

```{r}
colSums(is.na(data))[colSums(is.na(data)) != 0]

mod_df <- data.frame(data)

# dropping columns that will not affect our data analysis in any way
mod_df <- select(mod_df, -c('SOURCE_QUALITY':'OLD_ID'))
colSums(is.na(mod_df))[colSums(is.na(mod_df)) != 0]
colSums(is.na(mod_df))[colSums(is.na(mod_df)) == 0]
```

```{r}
# tables to summarize data
# find twelve variables that most interested in, and do correlatin matrix
# if certain variables are very highly correlated, then only use one of the two

# geom jitter -- points won't be laying on top of each other

for (i in 1:length(colnames(mod_df))) {
  column = colnames(mod_df)[i]
  if (sum(is.na(mod_df[, column])) == 0) {
    if (!(column %in% c("ID", "COUNTRY_NAME", "ISO2", "ID", "START_DATE", 
                        "END_DATE", "ISO3"))) {
      print(column)
      print(table(mod_df[, column]))
    }
  }
}
```


we know that there are 1762 observations total. we substitute out visa_ban (0 or 1 values)
with visa_ban_type, which encapsulates all, specific, or none -- we will need to one-hot encode this! other ones to explore: history_ban_list and citizen_list. If I use these, then eliminate history_ban and citizen from consideration (these are values that don't have N/As)

```{r}
# data cleaning for NA values

## VISA_BAN_LIST

colSums(is.na(mod_df))[colSums(is.na(mod_df)) != 0]

mod_df$VISA_BAN_NONE <- rep(0, nrow(mod_df))
mod_df[is.na(mod_df$VISA_BAN_TYPE), ]$VISA_BAN_NONE <- 1

mod_df$VISA_BAN_ALL <- rep(0, nrow(mod_df))
mod_df[mod_df$VISA_BAN_TYPE == "All" 
       & !is.na(mod_df$VISA_BAN_TYPE), ]$VISA_BAN_ALL <- 1

mod_df$VISA_BAN_SPECIFIC <- rep(0, nrow(mod_df))
mod_df[mod_df$VISA_BAN_TYPE == "specific" 
       & !is.na(mod_df$VISA_BAN_TYPE), ]$VISA_BAN_SPECIFIC <- 1

```

```{r}
## HISTORY_BAN_LIST

# for now, will count the number of commas
# it would be interesting to explore whether certain countries are banned more often than others, but I feel like the variation is too large that this would not be a productive use of my time

# helper function to determine the number of countries 
# i.e., number of commas plus one

country_counter <- function(obj) {
  if (is.na(obj)) {
    return(0)
  }
  return ((str_count(obj, ',')) + 1)
}

mod_df$HISTORY_BAN_CLEANED <- lapply(mod_df$HISTORY_BAN_LIST, country_counter)
mod_df$CITIZEN_LIST_CLEANED <- lapply(mod_df$CITIZEN_LIST, country_counter)
```

for clustering, will use 
- policy_type, (maybe policy_subtype?) -- need to one-hot-encode
- length of policy (end_date - start_date)
- air, land, sea, refugee, country_excep, work_excep
- visa_ban, citizen_list, and history_ban are already covered by the ``list" values we are including



```{r}
# data cleaning for non-NA values
colSums(is.na(mod_df))[colSums(is.na(mod_df)) == 0]

## DATES
mod_df$START_DATE_CLEANED <- as.Date(mod_df$START_DATE, tryFormats = "%m_%d_%y")
mod_df$END_DATE_CLEANED <- as.Date(mod_df$END_DATE, tryFormats = "%m_%d_%y")
# making assumption that "NA" end date means the policy is still in place
# na values --> setting them equal to today's date
mod_df[is.na(mod_df$END_DATE_CLEANED), ]$END_DATE_CLEANED <- Sys.Date()

# making (possibly faulty assumption) that the ``negative" policy lengths were never in place
# set these values equal to zero
mod_df$POLICY_LENGTH <- difftime(mod_df$END_DATE_CLEANED, mod_df$START_DATE_CLEANED, units = c("days"))
mod_df[mod_df$POLICY_LENGTH < 0 & !is.na(mod_df$POLICY_LENGTH), ]$POLICY_LENGTH <- 0
# no policy implemented will have start date of none --> need to set this to zero as well
mod_df[mod_df$POLICY_TYPE == "NOPOLICYIMPLEMENTED", ]$POLICY_LENGTH <- 0

```


```{r}
## one-hot encoding the policy type

# 0 --> not implemented, 1 --> partially implemented, 2 --> complete
mod_df$POLICY_TYPE_CLEANED <- rep(0, nrow(mod_df))
mod_df[mod_df$POLICY_TYPE == "PARTIAL", ]$POLICY_TYPE_CLEANED <- 1
mod_df[mod_df$POLICY_TYPE == "COMPLETE", ]$POLICY_TYPE_CLEANED <- 2

```


AT THIS POINT, WE ARE DONE WITH CLEANING. THESE ARE THE VARIABLE NAMES WE WANT TO USE:


ones we've cleaned:


VISA_BAN_NONE, VISA_BAN_SPECIFIC, VISA_BAN_ALL, HISTORY_BAN_CLEANED, CITIZEN_LIST_CLEANED, POLICY_LENGTH, POLICY_TYPE_CLEANED

ones we've left alone:

AIR, LAND, SEA, REFUGEE, COUNTRY_EXCEP, WORK_EXCEP

goals by next Wednesday:
- kMeans cluster on selected variables
- hierarchical cluster 
- (not needed by next Wednesday, but we can vary the number of clusters and where you stop on the dendrogram) -- can talk about this as next steps
- plot two variables from demographics -- then plot the clusters we previously generated (for immigration policies) -- this can be a wednesday goal!
- can also run the cluster algorithm on the demographics data -- does not need to be a wednesday goal
- WorldBank, Gap Minder (may have an R package!) -- other potential data sets for the demographic
- try different distance metrics to see how much the answer changes (how robust is it to that choice?)
- k-modes clustering -- better suited for categorical data
