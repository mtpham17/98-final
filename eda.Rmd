---
title: "Final Project EDA"
output: pdf_document
---


```{r}
library(mltools)
library(data.table)
library(dplyr)
library(stringr)
library(klaR)
library(gapminder)
library(ggplot2)
library(dendextend)
library(Hmisc)
library(mlbench)
library(caret)
library(factoextra)
library(NbClust)
library(fossil)
library(countrycode)
library(tidyverse)
library(ggrepel)
```

```{r}
data <- read.csv('data/immigration_policies/policy_list.csv')
# summary(data)

```

```{r}
colSums(is.na(data))[colSums(is.na(data)) != 0]

mod_df <- data.frame(data)

# dropping columns that will not affect our data analysis in any way
mod_df <- mod_df[, -c(32:44)]
colSums(is.na(mod_df))[colSums(is.na(mod_df)) != 0]
colSums(is.na(mod_df))[colSums(is.na(mod_df)) == 0]
```

```{r}
# tables to summarize data
# find twelve variables that most interested in, and do correlatin matrix
# if certain variables are very highly correlated, then only use one of the two

# geom jitter -- points won't be laying on top of each other

for (i in 1:length(colnames(mod_df))) {
  column = colnames(mod_df)[i]
  if (sum(is.na(mod_df[, column])) == 0) {
    if (!(column %in% c("ID", "COUNTRY_NAME", "ISO2", "ID", "START_DATE", 
                        "END_DATE", "ISO3"))) {
      print(column)
      print(table(mod_df[, column]))
    }
  }
}
```


we know that there are 1762 observations total. we substitute out visa_ban (0 or 1 values)
with visa_ban_type, which encapsulates all, specific, or none -- we will need to one-hot encode this! other ones to explore: history_ban_list and citizen_list. If I use these, then eliminate history_ban and citizen from consideration (these are values that don't have N/As)

```{r}
# data cleaning for NA values

## VISA_BAN_LIST

colSums(is.na(mod_df))[colSums(is.na(mod_df)) != 0]

mod_df$VISA_BAN_NONE <- rep(0, nrow(mod_df))
mod_df[is.na(mod_df$VISA_BAN_TYPE), ]$VISA_BAN_NONE <- 1

mod_df$VISA_BAN_ALL <- rep(0, nrow(mod_df))
mod_df[mod_df$VISA_BAN_TYPE == "All" 
       & !is.na(mod_df$VISA_BAN_TYPE), ]$VISA_BAN_ALL <- 1

mod_df$VISA_BAN_SPECIFIC <- rep(0, nrow(mod_df))
mod_df[mod_df$VISA_BAN_TYPE == "specific" 
       & !is.na(mod_df$VISA_BAN_TYPE), ]$VISA_BAN_SPECIFIC <- 1

mod_df$POLICY_TYPE_COMPLETE <- rep(0, nrow(mod_df))
mod_df[mod_df$POLICY_TYPE ==  "COMPLETE"
       & !is.na(mod_df$POLICY_TYPE), ]$POLICY_TYPE_COMPLETE <- 1

mod_df$POLICY_TYPE_PARTIAL <- rep(0, nrow(mod_df))
mod_df[mod_df$POLICY_TYPE ==  "PARTIAL"
       & !is.na(mod_df$POLICY_TYPE), ]$POLICY_TYPE_PARTIAL <- 1

mod_df$POLICY_TYPE_NON <- rep(0, nrow(mod_df))
mod_df[mod_df$POLICY_TYPE ==  "NOPOLICYIMPLEMENTED"
       & !is.na(mod_df$POLICY_TYPE), ]$POLICY_TYPE_NON <- 1

```

```{r}
## HISTORY_BAN_LIST

# for now, will count the number of commas
# it would be interesting to explore whether certain countries are banned more often than others, but I feel like the variation is too large that this would not be a productive use of my time

# helper function to determine the number of countries 
# i.e., number of commas plus one

country_counter <- function(obj) {
  if (is.na(obj)) {
    return(0)
  }
  return ((str_count(obj, ','))[1] + 1)
}

mod_df$HISTORY_BAN_CLEANED <- unlist(lapply(mod_df$HISTORY_BAN_LIST, country_counter))
mod_df$CITIZEN_LIST_CLEANED <- unlist(lapply(mod_df$CITIZEN_LIST, country_counter))
```

for clustering, will use 
- policy_type, (maybe policy_subtype?) -- need to one-hot-encode
- length of policy (end_date - start_date)
- air, land, sea, refugee, country_excep, work_excep
- visa_ban, citizen_list, and history_ban are already covered by the ``list" values we are including



```{r}
# data cleaning for non-NA values
colSums(is.na(mod_df))[colSums(is.na(mod_df)) == 0]

## DATES
mod_df$START_DATE_CLEANED <- as.Date(mod_df$START_DATE, tryFormats = "%m_%d_%y")
mod_df$END_DATE_CLEANED <- as.Date(mod_df$END_DATE, tryFormats = "%m_%d_%y")
# making assumption that "NA" end date means the policy is still in place
# na values --> setting them equal to today's date
mod_df[is.na(mod_df$END_DATE_CLEANED), ]$END_DATE_CLEANED <- Sys.Date()

# making (possibly faulty assumption) that the ``negative" policy lengths were never in place
# set these values equal to zero
mod_df$POLICY_LENGTH <- difftime(mod_df$END_DATE_CLEANED, mod_df$START_DATE_CLEANED, units = c("days"))
mod_df[mod_df$POLICY_LENGTH < 0 & !is.na(mod_df$POLICY_LENGTH), ]$POLICY_LENGTH <- 0
# no policy implemented will have start date of none --> need to set this to zero as well
mod_df[mod_df$POLICY_TYPE == "NOPOLICYIMPLEMENTED", ]$POLICY_LENGTH <- 0
mod_df$POLICY_LENGTH <- as.numeric(mod_df$POLICY_LENGTH)

```


```{r}
## one-hot encoding the policy type

# 0 --> not implemented, 1 --> partially implemented, 2 --> complete
mod_df$POLICY_TYPE_CLEANED <- rep(0, nrow(mod_df))
mod_df[mod_df$POLICY_TYPE == "PARTIAL", ]$POLICY_TYPE_CLEANED <- 1
mod_df[mod_df$POLICY_TYPE == "COMPLETE", ]$POLICY_TYPE_CLEANED <- 2

```


AT THIS POINT, WE ARE DONE WITH CLEANING. THESE ARE THE VARIABLE NAMES WE WANT TO USE:


ones we've cleaned:

VISA_BAN_NONE, VISA_BAN_SPECIFIC, VISA_BAN_ALL, HISTORY_BAN_CLEANED, CITIZEN_LIST_CLEANED, POLICY_LENGTH, POLICY_TYPE_CLEANED

ones we've left alone:

AIR, LAND, SEA, REFUGEE, COUNTRY_EXCEP, WORK_EXCEP

```{r}
# post data cleaning -- need to aggregate by country
vars <- c("COUNTRY_NAME", "ISO3", "VISA_BAN_NONE", "VISA_BAN_SPECIFIC", "VISA_BAN_ALL",
          "HISTORY_BAN_CLEANED", "CITIZEN_LIST_CLEANED", "POLICY_LENGTH",
          "POLICY_TYPE_COMPLETE", "POLICY_TYPE_PARTIAL", "AIR", "LAND", "SEA", 
          "POLICY_TYPE_NON", "REFUGEE", "COUNTRY_EXCEP", "WORK_EXCEP")

standardize <- function(col) {
  return((col - mean(col)) / sd(col))
}

cleaned_df <- subset(mod_df, select=vars)
ind <- sapply(cleaned_df, is.numeric)
cleaned_df[ind] <- lapply(cleaned_df[ind], standardize)

```


```{r}
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}

data_cor <- rcorr(as.matrix(cleaned_df[, 3:ncol(cleaned_df)]))
flattenCorrMatrix(data_cor$r, data_cor$P)
```


```{r}
set.seed(98)
# load the library
# calculate correlation matrix
correlationMatrix <- cor(cleaned_df[, 3:ncol(cleaned_df)])
# summarize the correlation matrix
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.60)
# print indexes of highly correlated attributes
print(highlyCorrelated)
```



```{r}
# hopefully ISO3 can be easily matched with other data sets
by_country <- aggregate(cbind(VISA_BAN_NONE, VISA_BAN_SPECIFIC, VISA_BAN_ALL,
                              HISTORY_BAN_CLEANED,
                              CITIZEN_LIST_CLEANED, POLICY_LENGTH, POLICY_TYPE_NON, 
                              POLICY_TYPE_COMPLETE, POLICY_TYPE_PARTIAL,
                              AIR, LAND, 
                              SEA, REFUGEE, COUNTRY_EXCEP, WORK_EXCEP)~ISO3, data = cleaned_df, mean)

```


NOW, we can work with the by_country data frame!!!




```{r}
summary(cleaned_df)
new_vars <- c("VISA_BAN_NONE", "VISA_BAN_SPECIFIC", "VISA_BAN_ALL",
          "HISTORY_BAN_CLEANED", "CITIZEN_LIST_CLEANED", "POLICY_LENGTH",
          "POLICY_TYPE_COMPLETE", "POLICY_TYPE_PARTIAL", "AIR", "LAND", "SEA", 
          "POLICY_TYPE_NON", "REFUGEE", "COUNTRY_EXCEP", "WORK_EXCEP")
```

goals by next Wednesday:
- kMeans cluster on selected variables
- hierarchical cluster 
- (not needed by next Wednesday, but we can vary the number of clusters and where you stop on the dendrogram) -- can talk about this as next steps
- plot two variables from demographics -- then plot the clusters we previously generated (for immigration policies) -- this can be a wednesday goal!
- can also run the cluster algorithm on the demographics data -- does not need to be a wednesday goal
- WorldBank, Gap Minder (may have an R package!) -- other potential data sets for the demographic
- try different distance metrics to see how much the answer changes (how robust is it to that choice?)
- k-modes clustering -- better suited for categorical data


- see how clusters change with inclusion of different variables



FEEDBACK FROM PRESENTATION:

- log of GDP, population to adjust the scale
- formal tests: 2-sample means on a metric between clusters
- PCA on demographic factors for ease of visualization
- formal test to determine how many clusters there are
- some way to score the different policies, and then see if there is a correlation between that and certain demographic covariates
- find some indicator of "natural" clustering -- do we see patterns among certain continents, developed vs developing, etc -- then adjust number of clusters based on the number of natural clusters, and see whether the contents of those clusters are the same
- try running PCA on immigration policies (???)

FOR MEETING WITH KELLY:
- have decided not to cluster countries based on their demographic factors, and to instead use that as a more informal way to investigate the clusters based on immigration policies

To do before meeting:
- download data on GDP, population, life expectancy, education rate, and fertility rate DONE
- officially decide on the number of clusters and linkage for HAC and K-means DONE
- the results section will consist of some visuals (probably PCA to get two dimensions -- but is this interpretable?), ANOVA test on those same factors (GDP, population, life expectancy, education rate, fertility rate) across different clusters (for both methods)
- look at the natural way of clustering (by continent, development level)

DETERMINING THE NUMBER OF CLUSTERS:

```{r}
# explain how number of clusters is very much impacted by choice of heuristics
jpeg(file="gap_k.jpg")
fviz_nbclust(by_country[,2:ncol(by_country)], kmeans, nstart = 25,  method = "gap_stat", 
             nboot =50)+ labs(subtitle = "Gap statistic method for K Means")
dev.off()

fviz_nbclust(by_country[,2:ncol(by_country)], kmeans, nstart = 25,  method = "silhouette", 
             nboot =50)+ labs(subtitle = "Silhouette method for K Means")
fviz_nbclust(by_country[,2:ncol(by_country)], kmeans, nstart = 25,  method = "wss", 
             nboot =50)+ labs(subtitle = "Elbow method for K Means")

jpeg(file="gap_hac.jpg")
fviz_nbclust(by_country[,2:ncol(by_country)], hcut, nstart = 25,  method = "gap_stat", 
             nboot = 50)+labs(subtitle = "Gap statistic method for HAC")
dev.off()
fviz_nbclust(by_country[,2:ncol(by_country)], hcut, nstart = 25,  method = "silhouette", 
             nboot = 50)+labs(subtitle = "Silhouette method for HAC")
fviz_nbclust(by_country[,2:ncol(by_country)], hcut, nstart = 25,  method = "wss", 
             nboot = 50)+labs(subtitle = "Elbow method for HAC")

# what to do if the two don't agree?
```


DETERMINING THE LINKAGE CRITERIA:

```{r}
# Data
dist_mat <- dist(by_country[,2:ncol(by_country)], method = 'euclidean')

# Hierarchical Agglomerative Clustering
h1=hclust(dist_mat,method='average')
h2=hclust(dist_mat,method='complete')
h4=hclust(dist_mat,method='single')

# Cophenetic Distances, for each linkage
c1=cophenetic(h1)
c2=cophenetic(h2)
c4=cophenetic(h4)

# Correlations
cor(dist_mat,c1) 
cor(dist_mat,c2) 
cor(dist_mat,c4)

# average is the best linkage method
```

for now, use 3 clusters (for HAC and k-means)

```{r}
# kmeans clustering
set.seed(98)
cluster.results.10 <- kmeans(by_country[,2:ncol(by_country)], 10, 
                          iter.max = 10, nstart = 1)

# cluster.results.6 <- kmeans(by_country[,2:ncol(by_country)], 6, 
#                           iter.max = 10, nstart = 1)

kcluster_by_country = data.frame(by_country)
kcluster_by_country$cluster10 <- as.factor(cluster.results.10$cluster)
# kcluster_by_country$cluster10 <- as.factor(cluster.results.6$cluster)
```


```{r}
# hierarchical clustering

dist_mat <- dist(by_country[,2:ncol(by_country)], method = 'euclidean')
hclust_avg <- hclust(dist_mat, method = 'average')

jpeg(file="cluster_den.jpg")
plot(hclust_avg)
dev.off()

cut_avg10 <- cutree(hclust_avg, k = 10)

avg_dend_obj <- as.dendrogram(hclust_avg, h = 10, leaflab = "none")
labels(avg_dend_obj) <- rep(NA, nrow(by_country))
avg_col_dend10 <- color_branches(avg_dend_obj, k = 10)

jpeg(file="cluster_den10.jpg")
plot(avg_col_dend10)
dev.off()

# jpeg(file="cluster_den6.jpg")
# plot(avg_col_dend6)
# dev.off()

hcluster_by_country10 <- mutate(by_country, cluster = cut_avg10)
# hcluster_by_country6 <- mutate(by_country, cluster = cut_avg6)

hcluster_by_country <- data.frame(by_country)

hcluster_by_country$cluster10 <- as.factor(hcluster_by_country10$cluster)
# hcluster_by_country$cluster10 <- as.factor(hcluster_by_country6$cluster)
```

```{r}
# bringing in demographic data; need life expectancy, literacy rate, and fertility rate
gdp <- read.csv('data/demographic/gdp.csv')
population <- read.csv('data/demographic/population.csv')
life_expectancy <- read.csv('data/demographic/life_expectancy.csv')
fertility_rate <- read.csv('data/demographic/fertility_rate.csv')
literacy_rate <- read.csv('data/demographic/literacy_rate.csv')
iso3 <- read.csv('data/demographic/iso3.csv')
```

```{r}
# some data cleaning on literacy rate -- need to note how not all of them were pulled from
# 2020
literacy_rate <- merge(literacy_rate, iso3, by.x = "country", by.y = "Country")
literacy_rate <- subset(literacy_rate, select = c(latestRate, Alpha.3.code))
colnames(literacy_rate) <- c('literacy', 'Code')
literacy_rate$Code <- trimws(literacy_rate$Code)
```


```{r}

master_df_k <- merge(kcluster_by_country, gdp, by.x = "ISO3", by.y = "Code")
master_df_k <- merge(master_df_k, population, by.x = "ISO3", by.y = "Code")
master_df_k <- merge(master_df_k, life_expectancy, by.x = "ISO3", by.y = "Code")
master_df_k <- merge(master_df_k, fertility_rate, by.x = "ISO3", by.y = "Code")
master_df_k <- merge(master_df_k, literacy_rate, by.x = "ISO3", by.y = "Code")
master_df_k <- subset(master_df_k, select = -c(Name.x, X.x, Name.y, X.y))

master_df_h <- merge(hcluster_by_country, gdp, by.x = "ISO3", by.y = "Code")
master_df_h <- merge(master_df_h, population, by.x = "ISO3", by.y = "Code")
master_df_h <- merge(master_df_h, life_expectancy, by.x = "ISO3", by.y = "Code")
master_df_h <- merge(master_df_h, fertility_rate, by.x = "ISO3", by.y = "Code")
master_df_h <- merge(master_df_h, literacy_rate, by.x = "ISO3", by.y = "Code")
master_df_h <- subset(master_df_h, select = -c(Name.x, X.x, Name.y, X.y))

```



```{r}
# anova stuff
# histograms or boxplots -- distribution of these variables across these clusters change
# for ones that are significant -- look for outliers!

k_gdp <- aov(GDP ~ cluster10, data = master_df_k)
k_pop <- aov(Pop ~ cluster10, data = master_df_k) 
k_exp <- aov(Expectancy ~ cluster10, data = master_df_k) 
k_fert <- aov(Fertility ~ cluster10, data = master_df_k) 
k_lit <- aov(literacy ~ cluster10, data = master_df_k) 

summary(k_gdp)
summary(k_pop)
summary(k_exp)
summary(k_fert)
summary(k_lit)
```

```{r}
h_gdp <- aov(GDP ~ cluster10, data = master_df_h)
h_pop <- aov(Pop ~ cluster10, data = master_df_h) 
h_exp <- aov(Expectancy ~ cluster10, data = master_df_h) 
h_fert <- aov(Fertility ~ cluster10, data = master_df_h) 
h_lit <- aov(literacy ~ cluster10, data = master_df_h) 

summary(h_gdp)
summary(h_pop)
summary(h_exp)
summary(h_fert)
summary(h_lit)
```

https://gist.github.com/tadast/8827699
https://worldpopulationreview.com/country-rankings/literacy-rate-by-country
https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/

```{r}
# need to merge continent and development level into the data
continents <- read.csv('data/demographic/continent.csv')
continents <- subset(continents, select = c(continent, code_3))
old <- c("Asia", "Europe", "Africa", "Oceania", "Americas")
new <- 1:length(old)
continents$continent[continents$continent %in% old] <- new[match(continents$continent, 
                                                                 old, nomatch = 0)]
continents$continent <- as.numeric(continents$continent)
master_df_k_continent <- merge(master_df_k, continents, by.x = "ISO3", by.y = "code_3")
master_df_h_continent <- merge(master_df_h, continents, by.x = "ISO3", by.y = "code_3")
```

HDI classifications are based on HDI fixed cutoff points, which are derived from the quartiles of dis- tributions of the component indicators. The cutoff-points are HDI of less than 0.550 for low human development, 0.550–0.699 for medium human development, 0.700–0.799 for high human development and 0.800 or greater for very high human development.

https://hdr.undp.org/en/content/human-development-report-2020-readers-guide


```{r}
hdi <- read.csv('data/demographic/hdi.csv')
hdi <- merge(hdi, iso3, by.x = "country", by.y = "Country")
hdi <- subset(hdi, select = c(hdi, Alpha.3.code))
colnames(hdi) <- c('hdi', 'Code')
hdi$development <- rep(1, nrow(hdi))
hdi[hdi$hdi >= 0.55 & hdi$hdi <= 0.699, ]$development <- 2
hdi[hdi$hdi >= 0.7 & hdi$hdi <= 0.799, ]$development <- 3
hdi[hdi$hdi >= 0.8, ]$development <- 4
hdi$Code <- trimws(hdi$Code)

master_df_k_hdi <- merge(master_df_k, hdi, by.x = "ISO3", by.y = "Code")
master_df_h_hdi <- merge(master_df_h, hdi, by.x = "ISO3", by.y = "Code")

```


```{r}
rand.index(as.numeric(levels(master_df_k_continent$cluster10))[master_df_k_continent$cluster10],
           master_df_k_continent$continent)
rand.index(as.numeric(levels(master_df_h_continent$cluster10))[master_df_h_continent$cluster10],
           master_df_k_continent$continent)
rand.index(as.numeric(levels(master_df_h_continent$cluster10))[master_df_h_continent$cluster10],
           as.numeric(levels(master_df_k_continent$cluster10))[master_df_k_continent$cluster10])
rand.index(as.numeric(levels(master_df_k_hdi$cluster10))[master_df_k_hdi$cluster10], 
           master_df_k_hdi$development)
rand.index(as.numeric(levels(master_df_h_hdi$cluster10))[master_df_h_hdi$cluster10], 
           master_df_h_hdi$development)
# trying to explain these results? need similar data for developed vs undeveloped
# be prepared to justify why!!
```

```{r}
# generating graphs: possible pairs

#       [,1]         [,2]        
#  [1,] "GDP"        "Pop"       
#  [2,] "GDP"        "Expectancy"
#  [3,] "GDP"        "Fertility" 
#  [4,] "GDP"        "literacy"  
#  [5,] "Pop"        "Expectancy"
#  [6,] "Pop"        "Fertility" 
#  [7,] "Pop"        "literacy"  
#  [8,] "Expectancy" "Fertility" 
#  [9,] "Expectancy" "literacy"  
# [10,] "Fertility"  "literacy"  

vars <- c("GDP", "Pop", "Expectancy", "Fertility", "literacy")
pairs <- t(combn(vars, 2))

p2 <- ggplot(master_df_k, aes(x = log(GDP), y = log(Pop), color = cluster10)) +
  geom_point(size=2)
p2 + ggtitle("K-Means: 10 Clusters") + scale_fill_brewer(palette="Set3")

p2 <- ggplot(master_df_k, aes(x = log(GDP), y = log(Expectancy), color = cluster10)) +
  geom_point(size=2)
p2 + ggtitle("K-Means: 10 Clusters") + scale_fill_brewer(palette="Set3")

p2 <- ggplot(master_df_k, aes(x = log(GDP), y = log(Fertility), color = cluster10)) +
  geom_point(size=2)
p2 + ggtitle("K-Means: 10 Clusters") + scale_fill_brewer(palette="Set3")

p2 <- ggplot(master_df_k, aes(x = log(GDP), y = log(literacy), color = cluster10)) +
  geom_point(size=2)
p2 + ggtitle("K-Means: 10 Clusters") + scale_fill_brewer(palette="Set3")

p2 <- ggplot(master_df_k, aes(x = log(GDP), y = log(Expectancy), color = cluster10)) +
  geom_point(size=2)
p2 + ggtitle("K-Means: 10 Clusters") + scale_fill_brewer(palette="Set3")

p2 <- ggplot(master_df_k, aes(x = log(Pop), y = log(Expectancy), color = cluster10)) +
  geom_point(size=2)
p2 + ggtitle("K-Means: 10 Clusters") + scale_fill_brewer(palette="Set3")

p2 <- ggplot(master_df_k, aes(x = log(Pop), y = log(Fertility), color = cluster10)) +
  geom_point(size=2)
p2 + ggtitle("K-Means: 10 Clusters") + scale_fill_brewer(palette="Set3")

p2 <- ggplot(master_df_k, aes(x = log(Pop), y = log(literacy), color = cluster10)) +
  geom_point(size=2)
p2 + ggtitle("K-Means: 10 Clusters") + scale_fill_brewer(palette="Set3")

p2 <- ggplot(master_df_k, aes(x = log(Expectancy), y = log(Fertility), color = cluster10)) +
  geom_point(size=2)
p2 + ggtitle("K-Means: 10 Clusters") + scale_fill_brewer(palette="Set3")

p2 <- ggplot(master_df_k, aes(x = log(Fertility), y = log(literacy), color = cluster10)) +
  geom_point(size=2)
p2 + ggtitle("K-Means: 10 Clusters") + scale_fill_brewer(palette="Set3")

#jpeg(file="kmeans_6.jpg")
# p2 <- ggplot(master_df_k, aes(x = log(Pop), y = log(Expectancy), color = cluster10)) + geom_point(size=2)
# p2 + ggtitle("K-Means: 6 Clusters") + scale_fill_brewer(palette="Set3")
#dev.off()

#jpeg(file="hac_6.jpg")
# p3 <- ggplot(master_df_h, aes(x = log(Pop), y = log(Expectancy), color = cluster10)) + geom_point(size=2) 
# p3 + ggtitle("HAC: 6 Clusters") + scale_fill_brewer(palette="Set3")
#dev.off()
```

```{r}
p2 <- ggplot(master_df_h, aes(x = log(GDP), y = log(Pop), color = cluster10)) +
  geom_point(size=2)
p2 + ggtitle("HAC: 10 Clusters") + scale_fill_brewer(palette="Set3")

p2 <- ggplot(master_df_h, aes(x = log(GDP), y = log(Expectancy), color = cluster10)) +
  geom_point(size=2)
p2 + ggtitle("HAC: 10 Clusters") + scale_fill_brewer(palette="Set3")

p2 <- ggplot(master_df_h, aes(x = log(GDP), y = log(Fertility), color = cluster10)) +
  geom_point(size=2)
p2 + ggtitle("HAC: 10 Clusters") + scale_fill_brewer(palette="Set3")

p2 <- ggplot(master_df_h, aes(x = log(GDP), y = log(literacy), color = cluster10)) +
  geom_point(size=2)
p2 + ggtitle("HAC: 10 Clusters") + scale_fill_brewer(palette="Set3")

p2 <- ggplot(master_df_h, aes(x = log(GDP), y = log(Expectancy), color = cluster10)) +
  geom_point(size=2)
p2 + ggtitle("HAC: 10 Clusters") + scale_fill_brewer(palette="Set3")

p2 <- ggplot(master_df_h, aes(x = log(Pop), y = log(Expectancy), color = cluster10)) +
  geom_point(size=2)
p2 + ggtitle("HAC: 10 Clusters") + scale_fill_brewer(palette="Set3")

p2 <- ggplot(master_df_h, aes(x = log(Pop), y = log(Fertility), color = cluster10)) +
  geom_point(size=2)
p2 + ggtitle("HAC: 10 Clusters") + scale_fill_brewer(palette="Set3")

p2 <- ggplot(master_df_h, aes(x = log(Pop), y = log(literacy), color = cluster10)) +
  geom_point(size=2)
p2 + ggtitle("HAC: 10 Clusters") + scale_fill_brewer(palette="Set3")

p2 <- ggplot(master_df_h, aes(x = log(Expectancy), y = log(Fertility), color = cluster10)) +
  geom_point(size=2)
p2 + ggtitle("HAC: 10 Clusters") + scale_fill_brewer(palette="Set3")

p2 <- ggplot(master_df_h, aes(x = log(Fertility), y = log(literacy), color = cluster10)) +
  geom_point(size=2)
p2 + ggtitle("HAC: 10 Clusters") + scale_fill_brewer(palette="Set3")
```


```{r}
jpeg(file="boxplot_k_exp.jpg")
p <- ggplot(master_df_k, aes(x=cluster10, y=Expectancy, color = cluster10)) + 
  geom_boxplot()
p
dev.off()

jpeg(file="boxplot_hac_gdp.jpg")
p <- ggplot(master_df_h, aes(x=cluster10, y=GDP, color = cluster10)) + 
  geom_boxplot()
p
dev.off()

jpeg(file="boxplot_k_fertility.jpg")
p <- ggplot(master_df_k, aes(x=cluster10, y=Fertility, color = cluster10)) + 
  geom_boxplot()
p
dev.off()

jpeg(file="boxplot_k_literacy.jpg")
p <- ggplot(master_df_k, aes(x=cluster10, y=literacy, color = cluster10)) + 
  geom_boxplot()
p
dev.off()


```

```{r}

# test <- subset(master_df_k, select=c(ISO3, cluster10, literacy))
# test$cluster10 <- as.numeric(levels(test$cluster10))[test$cluster10]
# 
# is_outlier <- function(x) {
#   return(x < quantile(x, 0.25) - 1.5 * IQR(x) | x > quantile(x, 0.75) + 1.5 * IQR(x))
# }
# 
# test %>%
# pivot_longer(names_to = "variable", values_to = "value", -ISO3) %>% 
# group_by(variable) %>% 
# mutate(outlier = if_else(is_outlier(value), ISO3, NA_character_)) %>% 
# ggplot(aes(x = variable, y = value, color = variable)) +
# geom_boxplot() +
# geom_text_repel(aes(label = outlier), na.rm = TRUE, show.legend = F) 
```


```{r}
# not sure how meaningful something like this would be? definitely needs some cleaning up
# source of code: https://stackoverflow.com/questions/47842646/labelling-outliers-with-ggplot
ggplot(master_df_k, aes(x = cluster10, y = literacy, fill = cluster10)) +
  geom_boxplot(alpha = 0.3) +
  geom_point(aes(color = cluster10, group = cluster10), position = position_dodge(width=0.75)) +
  geom_text(aes(group = cluster10, 
                label = ifelse(test = literacy > median(literacy) + 1.5*IQR(literacy)
                               | literacy < median(literacy) - 1.5*IQR(literacy), 
                  yes = ISO3,
                  no = '')), 
            position = position_dodge(width=0.75),
            hjust = "left", size = 3)
```

```{r}
ggplot(master_df_h[!is.na(master_df_h$GDP), ], aes(x = cluster10, y = GDP, fill = cluster10)) +
  geom_boxplot(alpha = 0.3) +
  geom_point(aes(color = cluster10, group = cluster10), position = position_dodge(width=0.75)) +
  geom_text(aes(group = cluster10, 
                label = ifelse(test = GDP > median(GDP) + 1.5*IQR(GDP)
                               | GDP < median(GDP) - 1.5*IQR(GDP), 
                  yes = ISO3,
                  no = '')), 
            position = position_dodge(width=0.75),
            hjust = "left", size = 3)
```

```{r}
ggplot(master_df_k, aes(x = cluster10, y = Expectancy, fill = cluster10)) +
  geom_boxplot(alpha = 0.3) +
  geom_point(aes(color = cluster10, group = cluster10), position = position_dodge(width=0.75)) +
  geom_text(aes(group = cluster10, 
                label = ifelse(test = Expectancy > median(Expectancy) + 1.5*IQR(Expectancy)
                               | Expectancy < median(Expectancy) - 1.5*IQR(Expectancy), 
                  yes = ISO3,
                  no = '')), 
            position = position_dodge(width=0.75),
            hjust = "left", size = 3)
```


```{r}

for (i in 1:10) {
  print(master_df_h[master_df_h$cluster10 == i, ]$ISO3)
}
```

```{r}
for (i in 1:10) {
  print(master_df_h[master_df_k$cluster10 == i, ]$ISO3)
}
```

questions:

- whether PCA from 5 to 2 variables is interpretable/useful in the first place -- can just choose a few sets of two variables that are interesting
- thinking of using Rand index to compare clustering: https://en.wikipedia.org/wiki/Rand_index (continent, development level)
- do I have enough? should I be 
- motivating the problem -- making sure I'm not just saying that it's interesting ``to me"
- don't have all the results yet, and even if I do, am afraid they are not meaningful enough
- end with what surprised



FINAL OFFICE HOURS:
- moving the tables in the data section to the appendix?
- standardized or original data summary statistics in the table? both?
- telling a meaningful story with the scatter plot?


