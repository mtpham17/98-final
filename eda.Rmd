---
title: "Final Project EDA"
output: pdf_document
---


```{r}
library(mltools)
library(data.table)
library(dplyr)
library(stringr)
library(klaR)
library(gapminder)
library(ggplot2)
library(dendextend)
library(Hmisc)
library(mlbench)
library(caret)
library(factoextra)
library(NbClust)
```

```{r}
data <- read.csv('data/immigration_policies/policy_list.csv')
# summary(data)

```

```{r}
colSums(is.na(data))[colSums(is.na(data)) != 0]

mod_df <- data.frame(data)

# dropping columns that will not affect our data analysis in any way
mod_df <- mod_df[, -c(32:44)]
colSums(is.na(mod_df))[colSums(is.na(mod_df)) != 0]
colSums(is.na(mod_df))[colSums(is.na(mod_df)) == 0]
```

```{r}
# tables to summarize data
# find twelve variables that most interested in, and do correlatin matrix
# if certain variables are very highly correlated, then only use one of the two

# geom jitter -- points won't be laying on top of each other

for (i in 1:length(colnames(mod_df))) {
  column = colnames(mod_df)[i]
  if (sum(is.na(mod_df[, column])) == 0) {
    if (!(column %in% c("ID", "COUNTRY_NAME", "ISO2", "ID", "START_DATE", 
                        "END_DATE", "ISO3"))) {
      print(column)
      print(table(mod_df[, column]))
    }
  }
}
```


we know that there are 1762 observations total. we substitute out visa_ban (0 or 1 values)
with visa_ban_type, which encapsulates all, specific, or none -- we will need to one-hot encode this! other ones to explore: history_ban_list and citizen_list. If I use these, then eliminate history_ban and citizen from consideration (these are values that don't have N/As)

```{r}
# data cleaning for NA values

## VISA_BAN_LIST

colSums(is.na(mod_df))[colSums(is.na(mod_df)) != 0]

mod_df$VISA_BAN_NONE <- rep(0, nrow(mod_df))
mod_df[is.na(mod_df$VISA_BAN_TYPE), ]$VISA_BAN_NONE <- 1

mod_df$VISA_BAN_ALL <- rep(0, nrow(mod_df))
mod_df[mod_df$VISA_BAN_TYPE == "All" 
       & !is.na(mod_df$VISA_BAN_TYPE), ]$VISA_BAN_ALL <- 1

mod_df$VISA_BAN_SPECIFIC <- rep(0, nrow(mod_df))
mod_df[mod_df$VISA_BAN_TYPE == "specific" 
       & !is.na(mod_df$VISA_BAN_TYPE), ]$VISA_BAN_SPECIFIC <- 1

mod_df$POLICY_TYPE_COMPLETE <- rep(0, nrow(mod_df))
mod_df[mod_df$POLICY_TYPE ==  "COMPLETE"
       & !is.na(mod_df$POLICY_TYPE), ]$POLICY_TYPE_COMPLETE <- 1

mod_df$POLICY_TYPE_PARTIAL <- rep(0, nrow(mod_df))
mod_df[mod_df$POLICY_TYPE ==  "PARTIAL"
       & !is.na(mod_df$POLICY_TYPE), ]$POLICY_TYPE_PARTIAL <- 1

mod_df$POLICY_TYPE_NON <- rep(0, nrow(mod_df))
mod_df[mod_df$POLICY_TYPE ==  "NOPOLICYIMPLEMENTED"
       & !is.na(mod_df$POLICY_TYPE), ]$POLICY_TYPE_NON <- 1

```

```{r}
## HISTORY_BAN_LIST

# for now, will count the number of commas
# it would be interesting to explore whether certain countries are banned more often than others, but I feel like the variation is too large that this would not be a productive use of my time

# helper function to determine the number of countries 
# i.e., number of commas plus one

country_counter <- function(obj) {
  if (is.na(obj)) {
    return(0)
  }
  return ((str_count(obj, ','))[1] + 1)
}

mod_df$HISTORY_BAN_CLEANED <- unlist(lapply(mod_df$HISTORY_BAN_LIST, country_counter))
mod_df$CITIZEN_LIST_CLEANED <- unlist(lapply(mod_df$CITIZEN_LIST, country_counter))
```

for clustering, will use 
- policy_type, (maybe policy_subtype?) -- need to one-hot-encode
- length of policy (end_date - start_date)
- air, land, sea, refugee, country_excep, work_excep
- visa_ban, citizen_list, and history_ban are already covered by the ``list" values we are including



```{r}
# data cleaning for non-NA values
colSums(is.na(mod_df))[colSums(is.na(mod_df)) == 0]

## DATES
mod_df$START_DATE_CLEANED <- as.Date(mod_df$START_DATE, tryFormats = "%m_%d_%y")
mod_df$END_DATE_CLEANED <- as.Date(mod_df$END_DATE, tryFormats = "%m_%d_%y")
# making assumption that "NA" end date means the policy is still in place
# na values --> setting them equal to today's date
mod_df[is.na(mod_df$END_DATE_CLEANED), ]$END_DATE_CLEANED <- Sys.Date()

# making (possibly faulty assumption) that the ``negative" policy lengths were never in place
# set these values equal to zero
mod_df$POLICY_LENGTH <- difftime(mod_df$END_DATE_CLEANED, mod_df$START_DATE_CLEANED, units = c("days"))
mod_df[mod_df$POLICY_LENGTH < 0 & !is.na(mod_df$POLICY_LENGTH), ]$POLICY_LENGTH <- 0
# no policy implemented will have start date of none --> need to set this to zero as well
mod_df[mod_df$POLICY_TYPE == "NOPOLICYIMPLEMENTED", ]$POLICY_LENGTH <- 0
mod_df$POLICY_LENGTH <- as.numeric(mod_df$POLICY_LENGTH)

```


```{r}
## one-hot encoding the policy type

# 0 --> not implemented, 1 --> partially implemented, 2 --> complete
mod_df$POLICY_TYPE_CLEANED <- rep(0, nrow(mod_df))
mod_df[mod_df$POLICY_TYPE == "PARTIAL", ]$POLICY_TYPE_CLEANED <- 1
mod_df[mod_df$POLICY_TYPE == "COMPLETE", ]$POLICY_TYPE_CLEANED <- 2

```


AT THIS POINT, WE ARE DONE WITH CLEANING. THESE ARE THE VARIABLE NAMES WE WANT TO USE:


ones we've cleaned:

VISA_BAN_NONE, VISA_BAN_SPECIFIC, VISA_BAN_ALL, HISTORY_BAN_CLEANED, CITIZEN_LIST_CLEANED, POLICY_LENGTH, POLICY_TYPE_CLEANED

ones we've left alone:

AIR, LAND, SEA, REFUGEE, COUNTRY_EXCEP, WORK_EXCEP

```{r}
# post data cleaning -- need to aggregate by country
vars <- c("COUNTRY_NAME", "ISO3", "VISA_BAN_NONE", "VISA_BAN_SPECIFIC", "VISA_BAN_ALL",
          "HISTORY_BAN_CLEANED", "CITIZEN_LIST_CLEANED", "POLICY_LENGTH",
          "POLICY_TYPE_COMPLETE", "POLICY_TYPE_PARTIAL", "AIR", "LAND", "SEA", 
          "POLICY_TYPE_NON", "REFUGEE", "COUNTRY_EXCEP", "WORK_EXCEP")

cleaned_df <- subset(mod_df, select=vars)

```


```{r}
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}

data_cor <- rcorr(as.matrix(cleaned_df[, 3:ncol(cleaned_df)]))
flattenCorrMatrix(data_cor$r, data_cor$P)
```


```{r}
set.seed(98)
# load the library
# calculate correlation matrix
correlationMatrix <- cor(cleaned_df[, 3:ncol(cleaned_df)])
# summarize the correlation matrix
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.60)
# print indexes of highly correlated attributes
print(highlyCorrelated)
```



```{r}
# hopefully ISO3 can be easily matched with other data sets
by_country <- aggregate(cbind(VISA_BAN_NONE, VISA_BAN_SPECIFIC, VISA_BAN_ALL,
                              HISTORY_BAN_CLEANED,
                              CITIZEN_LIST_CLEANED, POLICY_LENGTH, POLICY_TYPE_NON, 
                              POLICY_TYPE_COMPLETE, POLICY_TYPE_PARTIAL,
                              AIR, LAND, 
                              SEA, REFUGEE, COUNTRY_EXCEP, WORK_EXCEP)~ISO3, cleaned_df, mean)

```


NOW, we can work with the by_country data frame!!!




```{r}
summary(cleaned_df)
new_vars <- c("VISA_BAN_NONE", "VISA_BAN_SPECIFIC", "VISA_BAN_ALL",
          "HISTORY_BAN_CLEANED", "CITIZEN_LIST_CLEANED", "POLICY_LENGTH",
          "POLICY_TYPE_COMPLETE", "POLICY_TYPE_PARTIAL", "AIR", "LAND", "SEA", 
          "POLICY_TYPE_NON", "REFUGEE", "COUNTRY_EXCEP", "WORK_EXCEP")
```

goals by next Wednesday:
- kMeans cluster on selected variables
- hierarchical cluster 
- (not needed by next Wednesday, but we can vary the number of clusters and where you stop on the dendrogram) -- can talk about this as next steps
- plot two variables from demographics -- then plot the clusters we previously generated (for immigration policies) -- this can be a wednesday goal!
- can also run the cluster algorithm on the demographics data -- does not need to be a wednesday goal
- WorldBank, Gap Minder (may have an R package!) -- other potential data sets for the demographic
- try different distance metrics to see how much the answer changes (how robust is it to that choice?)
- k-modes clustering -- better suited for categorical data


- see how clusters change with inclusion of different variables



FEEDBACK FROM PRESENTATION:

- log of GDP, population to adjust the scale
- formal tests: 2-sample means on a metric between clusters
- PCA on demographic factors for ease of visualization
- formal test to determine how many clusters there are
- some way to score the different policies, and then see if there is a correlation between that and certain demographic covariates
- find some indicator of "natural" clustering -- do we see patterns among certain continents, developed vs developing, etc -- then adjust number of clusters based on the number of natural clusters, and see whether the contents of those clusters are the same
- try running PCA on immigration policies (???)

FOR MEETING WITH KELLY:
- have decided not to cluster countries based on their demographic factors, and to instead use that as a more informal way to investigate the clusters based on immigration policies

To do before meeting:
- download data on GDP, population, life expectancy, education rate, and fertility rate
- officially decide on the number of clusters and linkage for HAC and K-means DONE
- the results section will consist of some visuals (probably PCA to get two dimensions -- but is this interpretable?), ANOVA test on those same factors (GDP, population, life expectancy, education rate, fertility rate) across different clusters (for both methods)
- look at the natural way of clustering (by continent, development level)

DETERMINING THE NUMBER OF CLUSTERS:

```{r}
fviz_nbclust(by_country[,2:ncol(by_country)], kmeans, nstart = 25,  method = "gap_stat", 
             nboot =50)+ labs(subtitle = "Gap statistic method for K Means")
fviz_nbclust(by_country[,2:ncol(by_country)], kmeans, nstart = 25,  method = "silhouette", 
             nboot =50)+ labs(subtitle = "Silhouette method for K Means")
fviz_nbclust(by_country[,2:ncol(by_country)], kmeans, nstart = 25,  method = "wss", 
             nboot =50)+ labs(subtitle = "Elbow method for K Means")

fviz_nbclust(by_country[,2:ncol(by_country)], hcut, nstart = 25,  method = "gap_stat", 
             nboot = 50)+labs(subtitle = "Gap statistic method for HAC")
fviz_nbclust(by_country[,2:ncol(by_country)], hcut, nstart = 25,  method = "silhouette", 
             nboot = 50)+labs(subtitle = "Silhouette method for HAC")
fviz_nbclust(by_country[,2:ncol(by_country)], hcut, nstart = 25,  method = "wss", 
             nboot = 50)+labs(subtitle = "Elbow method for HAC")

# what to do if the two don't agree?
```


DETERMINING THE LINKAGE CRITERIA:

```{r}
# Data
dist_mat <- dist(by_country[,2:ncol(by_country)], method = 'euclidean')

# Hierarchical Agglomerative Clustering
h1=hclust(dist_mat,method='average')
h2=hclust(dist_mat,method='complete')
h4=hclust(dist_mat,method='single')

# Cophenetic Distances, for each linkage
c1=cophenetic(h1)
c2=cophenetic(h2)
c4=cophenetic(h4)

# Correlations
cor(dist_mat,c1) 
cor(dist_mat,c2) 
cor(dist_mat,c4)
```

for now, use 3 clusters (for HAC and k-means)

```{r}
# kmeans clustering
cluster.results.3 <- kmeans(by_country[,2:ncol(by_country)], 3, 
                          iter.max = 10, nstart = 1)

# cluster.results.6 <- kmeans(by_country[,2:ncol(by_country)], 6, 
#                           iter.max = 10, nstart = 1)

kcluster_by_country = data.frame(by_country)
kcluster_by_country$cluster3 <- as.factor(cluster.results.3$cluster)
# kcluster_by_country$cluster6 <- as.factor(cluster.results.6$cluster)
```


```{r}
# hierarchical clustering

dist_mat <- dist(by_country[,2:ncol(by_country)], method = 'euclidean')
hclust_avg <- hclust(dist_mat, method = 'single')

jpeg(file="cluster_den.jpg")
plot(hclust_avg)
dev.off()

cut_avg3 <- cutree(hclust_avg, k = 3)
# cut_avg6 <- cutree(hclust_avg, k = 6)

avg_dend_obj <- as.dendrogram(hclust_avg)
avg_col_dend3 <- color_branches(avg_dend_obj, k = 3)
# avg_col_dend6 <- color_branches(avg_dend_obj, k = 6)

jpeg(file="cluster_den3.jpg")
plot(avg_col_dend3)
dev.off()

# jpeg(file="cluster_den6.jpg")
# plot(avg_col_dend6)
# dev.off()


hcluster_by_country3 <- mutate(by_country, cluster = cut_avg3)
# hcluster_by_country6 <- mutate(by_country, cluster = cut_avg6)

hcluster_by_country <- data.frame(by_country)

hcluster_by_country$cluster3 <- as.factor(hcluster_by_country3$cluster)
# hcluster_by_country$cluster6 <- as.factor(hcluster_by_country6$cluster)
```

```{r}
# bringing in demographic data; need life expectancy, literacy rate, and fertility rate
gdp <- read.csv('data/demographic/gdp.csv')
population <- read.csv('data/demographic/population.csv')
life_expectancy <- read.csv('data/demographic/life_expectancy.csv')
fertility_rate <- read.csv('data/demographic/fertility_rate.csv')
```

```{r}

master_df_k <- merge(kcluster_by_country, gdp, by.x = "ISO3", by.y = "Code")
master_df_k <- merge(master_df_k, population, by.x = "ISO3", by.y = "Code")
master_df_k <- subset(master_df_k, select = -c(X.x, X.y, Name.x, Name.y))

master_df_h <- merge(hcluster_by_country, gdp, by.x = "ISO3", by.y = "Code")
master_df_h <- merge(master_df_h, population, by.x = "ISO3", by.y = "Code")
master_df_h <- subset(master_df_h, select = -c(X.x, X.y, Name.x, Name.y))

```



```{r}
# plot(master_df_k$GDP, master_df_k$Pop, col = master_df_k$cluster3)
# plot(jitter(master_df_k$Pop), master_df_k$GDP, pch = 16, col = master_df_k$cluster3)

jpeg(file="kmeans_3.jpg")
p1 <- ggplot(master_df_k, aes(x = log(Pop), y = log(GDP), color = cluster3)) + geom_point(size=2) 
p1 + ggtitle("K-Means: 3 Clusters") + scale_fill_brewer(palette="Set3")
dev.off()

# jpeg(file="kmeans_6.jpg")
# p2 <- ggplot(master_df_k, aes(x = log(Pop), y = log(GDP), color = cluster6)) + geom_point(size=2) 
# p2 + ggtitle("K-Means: 6 Clusters") + scale_fill_brewer(palette="Set3")
# dev.off()

jpeg(file="hac_3.jpg")
p3 <- ggplot(master_df_h, aes(x = log(Pop), y = log(GDP), color = cluster3)) + geom_point(size=2) 
p3 + ggtitle("HAC: 3 Clusters") + scale_fill_brewer(palette="Set3")
dev.off()

# jpeg(file="hac_6.jpg")
# p4 <- ggplot(master_df_h, aes(x = log(Pop), y = log(GDP), color = cluster6)) + geom_point(size=2) 
# p4 + ggtitle("HAC: 6 Clusters") + scale_fill_brewer(palette="Set3")
# dev.off()


```



